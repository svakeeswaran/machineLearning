{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier():\n",
    "    def __init__(self,k):\n",
    "        self.k = k\n",
    "        \n",
    "    def euclidean_distance(self,X1,X2):\n",
    "        if len(X1) != len(X2):\n",
    "            return -1\n",
    "        else:\n",
    "            ss = 0\n",
    "            for i in range(len(X1)):\n",
    "                ss+= (X1[i] - X2[i]) ** 2\n",
    "            distance = ss ** (0.5)\n",
    "        return distance\n",
    "    \n",
    "    def vote(self,index,values):\n",
    "        count = defaultdict(int)\n",
    "        for i in index:\n",
    "            count[values[i]]+=1\n",
    "        predProba = {}\n",
    "        pred = 0\n",
    "        maxi = max(count.values())\n",
    "        for key,val in count.items():\n",
    "            predProba[key] = val/self.k\n",
    "            if val == maxi:\n",
    "                pred = key\n",
    "        return {'pred':pred,'predProba':predProba}\n",
    "    \n",
    "    def smallestKDistanceIndex(self,distance):\n",
    "        distIndex = defaultdict(list)\n",
    "        for i in range(len(distance)):\n",
    "            distIndex[distance[i]].append(i)\n",
    "        final = []\n",
    "        keys = sorted(distIndex.keys())\n",
    "        for key in keys:\n",
    "            final.extend(distIndex[key])\n",
    "        print(final[:self.k])\n",
    "        return final[:self.k]\n",
    "                \n",
    "    def predict(self,X_train,Y_train,X_test):\n",
    "        ans = {}\n",
    "        for testIndex in range(len(X_test)):\n",
    "            distance = [0] * len(X_train)\n",
    "            for trainIndex in range(len(X_train)):\n",
    "                distance[trainIndex] = self.euclidean_distance(X_test[testIndex],X_train[trainIndex])\n",
    "            print(distance)\n",
    "            minimumDistanceIndex = self.smallestKDistanceIndex(distance)\n",
    "            ans[testIndex] = self.vote(minimumDistanceIndex,Y_train)\n",
    "        \n",
    "        return ans\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression:\n",
    "    def __init__(self,lr):\n",
    "        self.learning_rate = lr\n",
    "        self.params = None\n",
    "    \n",
    "    def fit(self,X_train,Y_train,n_epochs):\n",
    "        n_features = X_train.shape[1]\n",
    "        n_training = X_train.shape[0]\n",
    "        W = np.zeros((n_features,1))\n",
    "        B = 0\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            Y_dash = np.dot(X_train,W) + B\n",
    "            difference = np.subtract(Y_dash,Y_train)\n",
    "            SSE = np.sum(np.square(difference))\n",
    "            MSE = SSE/n_training\n",
    "            dW = np.dot(X_train.T,difference)/n_training\n",
    "            dB = np.sum(difference)/n_training\n",
    "            W = W - self.learning_rate * dW\n",
    "            B = B - self.learning_rate * dB\n",
    "        \n",
    "        self.params = (W,B)\n",
    "            \n",
    "    def predict(self,X_test):\n",
    "        W,B = self.params\n",
    "        y_dash = np.dot(X_test,W) +B\n",
    "        \n",
    "        return y_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = linearRegression(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1],[2],[3]])\n",
    "Y_train = np.array([[1],[3],[-1],[2]])\n",
    "X_test = np.array([[1,-1,2],[2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(Y_train,Y_train,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self,lr):\n",
    "        self.learning_rate = lr\n",
    "        self.params = None\n",
    "        \n",
    "    def fit(self,X_train,Y_train,n_epochs):\n",
    "        n_features = X_train.shape[1]\n",
    "        n_training = X_train.shape[0]\n",
    "        W = np.zeros((n_features,1))\n",
    "        B = 0\n",
    "        for i in range(n_epochs):\n",
    "            logit = np.dot(X_train,W) + B\n",
    "            Y_dash = 1/(1+np.exp(-1 * logit))\n",
    "            loss = np.sum((-Y_train * np.log(Y_dash)) - (1-Y_train) * np.log(1-Y_dash))\n",
    "            mean_loss = loss/n_training\n",
    "            dW = np.dot(X_train.T,(Y_dash-Y_train))/n_training\n",
    "            dB = np.sum(Y_dash-Y_train)/n_training\n",
    "            W = W - self.learning_rate * dW\n",
    "            B = B - self.learning_rate * dB\n",
    "        \n",
    "        self.params = (W,B)\n",
    "        \n",
    "    def predict(self,X_test):\n",
    "        W,B = self.params\n",
    "        logit = np.dot(X_test,W) + B\n",
    "        Y_dash = 1/(1+np.exp(-1 * logit))\n",
    "        return Y_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(10,4)\n",
    "Y = np.random.randint(low=2,size=(10,1))\n",
    "logistic = LogisticRegression(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.fit(X,Y,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.07014501e-01],\n",
       "       [  9.39531045e-01],\n",
       "       [  5.92793667e-01],\n",
       "       [  6.96543105e-01],\n",
       "       [  6.62364511e-01],\n",
       "       [  9.90736886e-01],\n",
       "       [  6.59826267e-01],\n",
       "       [  4.59648365e-02],\n",
       "       [  2.40951124e-01],\n",
       "       [  1.83664170e-04]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split n points into k clusters\n",
    "# Choose k centroids in random\n",
    "# Calculate distance from each point to each centroid\n",
    "# Calculate the average of points in a cluster to put a new centroid\n",
    "# Number of epochs\n",
    "# (10,5)-> (100,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    def __init__(self,k):\n",
    "        self.k = k\n",
    "        self.centroid = None\n",
    "        self.cluster = None\n",
    "        \n",
    "    def fit(self,X_train,n_epochs):\n",
    "        n_features = X_train.shape[1]\n",
    "        n_training = X_train.shape[0]\n",
    "        \n",
    "        indices = np.random.choice(replace=False,size=self.k,a=n_training)\n",
    "        centroid = X_train[indices]\n",
    "        print(centroid)\n",
    "        clusters = defaultdict(list)\n",
    "        for epoch in range(n_epochs):\n",
    "            clusters = defaultdict(list)\n",
    "            for t in range(len(X_train)):\n",
    "                distance = [0] * len(centroid)\n",
    "                for c in range(len(centroid)):\n",
    "                    distance[c] = np.linalg.norm(X_train[t] - centroid[c])\n",
    "                cl = distance.index(min(distance))\n",
    "                clusters[cl].append(X_train[t])\n",
    "            for key,val in clusters.items():\n",
    "                centroid[key] = np.average(val,axis=0)\n",
    "        \n",
    "        self.centroid = centroid\n",
    "        self.cluster = clusters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeansClustering(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array([[1,2],[2,4],[4,6],[6,7],[8,10],[11,14],[12,16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.28571429,  8.42857143])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 16]\n",
      " [11 14]\n",
      " [ 8 10]]\n"
     ]
    }
   ],
   "source": [
    "kmeans.fit(x,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [array([11, 14]), array([12, 16])],\n",
       "             1: [array([6, 7]), array([ 8, 10])],\n",
       "             2: [array([1, 2]), array([2, 4]), array([4, 6])]})"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 15],\n",
       "       [ 7,  8],\n",
       "       [ 2,  4]])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.centroid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
